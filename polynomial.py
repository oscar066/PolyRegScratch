# -*- coding: utf-8 -*-
"""Polynomial.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LA1zn5AeQcpRv10-zY9ktGnIKOx7_db0
"""

from google.colab import drive 
drive.mount('/content/drive')

!ls

!ls /content/drive/

"""### Importing libraries"""

# import libraries
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

sns.set_style('whitegrid')

# loading the data
df_train = pd.read_csv("/content/drive/MyDrive/kaggle_linear_regression_data/train.csv")
df_test =  pd.read_csv("/content/drive/MyDrive/kaggle_linear_regression_data/test.csv")
df_train.head(10)

df_train.info()

# 1.checking and dealing with missing values
def check_and_handle_missing(df):
    # check for missing values
    missing_values = df.isnull().sum()
    print("Missing values: ",missing_values[missing_values > 0],'\n')

    # drop the missing values
    df.dropna(inplace=True)
    return df

check_and_handle_missing(df_train)

# checking data was cleaned
df_train.isna().sum()

"""### Cleaning the data"""

# 2.check and handle duplicates
def check_and_handle_duplicates(df):
    # check for duplicates
    print("Total duplicates",df.duplicated().sum(),'\n')

    # drop duplicates
    df.drop_duplicates(inplace=True)
    return df

check_and_handle_duplicates(df_train)

# splittting the train data into X_train and y_train
X_train = df_train['x']
y_train = df_train['y']

# splitting the test data into X_test and y_test
X_test = df_test['x']
y_test = df_test['y']

# checking the shape of the data
print('X_train shape:',X_train.shape)
print('X_train shape:',y_train.shape)

print('X_test shape:' ,X_test.shape)
print('X_test shape:' ,y_test.shape)

# Reshaping the data
X_train = X_train.values.reshape(-1,1)
y_train = y_train.values.reshape(-1,1)

X_test = X_test.values.reshape(-1,1)
y_test = y_test.values.reshape(-1,1)

# checking the shape of the data
print('X_train shape:',X_train.shape)
print('X_train shape:',y_train.shape)

print('X_test shape:' ,X_test.shape)
print('X_test shape:' ,y_test.shape)

"""### Scaling the Data"""

# scaling the data
from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
y_train = scaler.fit_transform(y_train)

"""### Model Building"""

# implementing polynomial regression from scratch
class PolynomialRegression:
    def __init__(self, degree):
        self.degree = degree
        self.coef = None

    def fit(self, X, y):
        X = np.array(X)
        y = np.array(y)
        X = np.column_stack([X**i for i in range(self.degree+1)])
        self.coef = np.linalg.inv(X.T @ X) @ X.T @ y

    def predict(self, X):
        X = np.array(X)
        X = np.column_stack([X**i for i in range(self.degree+1)])
        return X @ self.coef

# training the model and overfitting it
model = PolynomialRegression(degree=15)
model.fit(X_train, y_train)

# predicting the values
y_pred_base = model.predict(X_test)

"""Training the model with a high degree to overfit the data"""

# plotting the results
fig = plt.figure(figsize=(10, 6))
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, y_pred_base, color='blue')
plt.title('Polynomial Regression', fontsize=15)
plt.xlabel('X', fontsize=15)
plt.ylabel('y', fontsize=15)
plt.show()

"""### Model Evaluation"""

# calculating the mean squared error
from sklearn.metrics import mean_squared_error

mse_base_poly = mean_squared_error(y_test, y_pred_base)
print('Mean Squared Error:',mse_base_poly)

"""### Regularization with Lasso and Ridge"""

# regularizing the model
from sklearn.linear_model import Lasso
ridge = Lasso(alpha=0.1)
ridge.fit(X_train, y_train)

# predicting the values
y_pred_lasso = ridge.predict(X_test)

# plotting the results
fig , ax = plt.subplots(figsize=(10,6))
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, y_pred_lasso, color='blue')
plt.title('Lasso Polynomial Regression',fontsize=15)
plt.xlabel('X',fontsize=15)
plt.ylabel('y',fontsize=15)
plt.show()

# evaluating the model
mse_lasso = mean_squared_error(y_test, y_pred_lasso)
print('Mean Squared Error for Lasso Model:',mse_lasso)

# regularizing the model  with ridge
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=0.1)
ridge.fit(X_train, y_train)

# predicting the values
y_pred_ridge = ridge.predict(X_test)

# plotting the results
fig , ax = plt.subplots(figsize=(10,6))
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, y_pred_ridge, color='blue')
plt.title('Ridge Polynomial Regression',fontsize=15)
plt.xlabel('X',fontsize=15)
plt.ylabel('y',fontsize=15)
plt.show()

# evaluating the model
mse_ridge = mean_squared_error(y_test, y_pred_ridge)
print('Mean Squared Error for Ridge Model:',mse_ridge)

# creating an ensemble model with both lasso and ridge regression
from sklearn.linear_model import Lasso, Ridge, LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.ensemble import VotingRegressor
from sklearn.pipeline import make_pipeline

# creating the models
model1 = Lasso(alpha=0.1)
model2 = Ridge(alpha=0.1)
model3 = make_pipeline(PolynomialFeatures(2), LinearRegression())

# creating the ensemble model
ensemble = VotingRegressor(estimators=[('lasso', model1), ('ridge', model2),('poly',model3)], weights=[1,1,1])

# training the ensemble model
ensemble.fit(X_train, y_train)

# predicting the values
y_pred_ensemble = ensemble.predict(X_test)

# plotting the results
fig , ax = plt.subplots(figsize=(10,6))
plt.scatter(X_test, y_test, color='red')
plt.plot(X_test, y_pred_ensemble, color='blue')
plt.title('Ensemble Regression',fontsize=15)
plt.xlabel('X',fontsize=15)
plt.ylabel('y',fontsize=15)
plt.show()

# evaluating the model
mse_ensemble = mean_squared_error(y_test, y_pred_ensemble)
print('Mean Squared Error for Ensemble Model:',mse_ensemble)

"""After regularization, the models were not overfitting the data and the model that performed the best was the Ridge model."""